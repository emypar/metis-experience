{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB Fetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import gzip\n",
    "import hashlib\n",
    "import time\n",
    "import random\n",
    "from http import HTTPStatus\n",
    "from urllib.parse import \\\n",
    "    urlparse\n",
    "import multiprocessing as mp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cache dir location:\n",
    "cache_dir = 'data/.web_fetcher'\n",
    "# Number of buckets for each site cache:\n",
    "num_buckets = 1000\n",
    "# If a dir like URL is fecthed (path ends in '/') then use this suffix to\n",
    "# store the results:\n",
    "dir_url_suffix = '__dir'\n",
    "# If path is null then use this file name to store the result:\n",
    "null_path_replace = '__nopath'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_url_to_cache_file(url, cache_dir=cache_dir, num_buckets=num_buckets, \n",
    "                          dir_url_suffix=dir_url_suffix,\n",
    "                          null_path_replace=null_path_replace):\n",
    "    \"\"\"Map URL to cache file path using site grouping and hashing\n",
    "       to avoid too many files per directory.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Save URL in:\n",
    "    #   cache_dir/netloc/scheme[/port]/bucket_num/path\n",
    "    \n",
    "    bucket_num = '{:08x}'.format(\n",
    "        int.from_bytes(hashlib.md5(url.encode()).digest(),\n",
    "                       byteorder = sys.byteorder) % num_buckets\n",
    "    )\n",
    "\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    url_netloc = parsed_url.netloc\n",
    "    url_port = ''\n",
    "    i = url_netloc.rfind(':')\n",
    "    if i >= 0:\n",
    "        url_port = url_netloc[i+1:]\n",
    "        url_netloc = url_netloc[:i]\n",
    "\n",
    "    url_path = parsed_url.path\n",
    "    if url_path:\n",
    "        if url_path[-1] == '/':\n",
    "            url_path = url_path[:-1] + dir_url_suffix\n",
    "        if url_path[0] == '/':\n",
    "            url_path = url_path[1:]\n",
    "    else:\n",
    "        url_path = null_path_replace\n",
    " \n",
    "    return os.path.join(cache_dir, \n",
    "                        url_netloc,\n",
    "                        parsed_url.scheme,\n",
    "                        url_port,\n",
    "                        bucket_num, \n",
    "                        url_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_url(url, retries=3, pause_between_retries=10, timeout=30):\n",
    "    \n",
    "    for i in range(retries):\n",
    "        if i > 0 and pause_between_retries > 0:\n",
    "            time.sleep((1 + 05. * random.random()) * pause_between_retries)\n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout)\n",
    "            if response.status_code == HTTPStatus.OK:\n",
    "                return response\n",
    "            elif response.status_code in [HTTPStatus.FORBIDDEN]:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        except requests.exceptions.Timeout:\n",
    "            pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_url(url, check_cache_only=False, seed_cache_only=False,\n",
    "              force_refresh=False):\n",
    "    \n",
    "    cache_file = map_url_to_cache_file(url)\n",
    "    \n",
    "    found_file = None\n",
    "    for f in [cache_file, cache_file + '.gz']:\n",
    "        if os.path.isfile(f):\n",
    "            found_file = f\n",
    "            break\n",
    "    \n",
    "    if check_cache_only:\n",
    "        return found_file\n",
    "    \n",
    "    if not found_file or force_refresh:\n",
    "        response = get_url(url)\n",
    "        if not response:\n",
    "            return None\n",
    "        found_file = cache_file + '.gz'\n",
    "        file_dir = os.path.dirname(found_file)\n",
    "        try:\n",
    "            if not os.path.isdir(file_dir):\n",
    "                os.makedirs(file_dir)\n",
    "        except FileExistsError:\n",
    "            # The crawler may run parallelized, so multiple threads/process \n",
    "            # instances may try to create it at the same time. Simply ignore\n",
    "            # the error since it doesn't matter who creates it.\n",
    "            pass\n",
    "        with gzip.open(found_file, mode='wb') as f:\n",
    "            f.write(response.text.encode())\n",
    "\n",
    "    if seed_cache_only:\n",
    "        return found_file\n",
    "    \n",
    "    open_func = gzip.open if found_file.endswith('.gz') else open\n",
    "    with open_func(found_file) as f:\n",
    "        content = f.read().decode()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seed_cache(url):\n",
    "    return (fetch_url(url, seed_cache_only=True) != None)\n",
    "\n",
    "def parallel_fetch(url_list, num_workers=10):\n",
    "    with mp.Pool(num_workers) as pool:\n",
    "        results = pool.map(seed_cache, url_list)\n",
    "    return len(list(filter(None, results)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
